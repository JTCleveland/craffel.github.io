<html><head><meta charset="UTF-8"><meta http-equiv="content-type" content="text/html;charset=UTF-8"><style>body{max-width:680px;margin:auto;padding:20px;text-align:justify;line-height:140%; -webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;font-smoothing:antialiased;color:#222;font-family:Palatino,Georgia,"Times New Roman",serif}</style><style>body{counter-reset: h1 h2 h3 h4 h5 h6 paragraph}@page{margin:0;size:auto}.md code,pre{font-family:Menlo,Consolas,monospace;font-size:13.456847078400001px;line-height:140%}.md div.title{font-size:26px;font-weight:800;line-height:120%;text-align:center}.md div.afterTitles{height:10px}.md div.subtitle{text-align:center}.md .image{display:inline-block}.md img{max-width:100%;page-break-inside:avoid}.md li{text-align:left;text-indent:0}.md pre.listing {tab-size:4;-moz-tab-size:4;-o-tab-size:4;counter-reset:line}.md pre.listing .linenumbers span.line:before{width:30px;margin-left:-52px;font-size:80%;text-align:right;counter-increment:line;content:counter(line);display:inline-block;padding-right:13px;margin-right:8px;color:#ccc}.md div.tilde{margin:20px 0 -10px;text-align:center}.md div.imagecaption,.md div.tablecaption,.md div.listingcaption{margin:7px 5px 12px;text-align: justify;font-style:italic}.md div.imagecaption{margin-bottom:0}.md blockquote.fancyquote{margin:25px 0 25px;text-align:left;line-height:160%}.md blockquote.fancyquote::before{content:"“";color:#DDD;font-family:Times New Roman;font-size:45px;line-height:0;margin-right:6px;vertical-align:-0.3em}.md span.fancyquote{font-size:118%;color:#777;font-style:italic}.md span.fancyquote::after{content:"”";font-style:normal;color:#DDD;font-family:Times New Roman;font-size:45px;line-height:0;margin-left:6px;vertical-align:-0.3em}.md blockquote.fancyquote .author{width:100%;margin-top:10px;display:inline-block;text-align:right}.md small{font-size:60%}.md div.title,contents,.md .tocHeader,h1,h2,h3,h4,h5,h6,.md .shortTOC,.md .mediumTOC,.nonumberh1,.nonumberh2,.nonumberh3,.nonumberh4,.nonumberh5,.nonumberh6{font-family:Verdana,Helvetica,Arial,sans-serif;margin:13.4px 0 13.4px;padding:15px 0 3px;border-top:none;clear:both}.md h1,.md h2,.md h3,.md h4,.md h5,.md h6,.md .nonumberh1,.md .nonumberh2,.md .nonumberh3,.md .nonumberh4,.md .nonumberh5,.md .nonumberh6{page-break-after:avoid;break-after:avoid}.md svg.diagram{display:block;font-family:Menlo,Consolas,monospace;font-size:13.456847078400001px;text-align:center;stroke-linecap:round;stroke-width:2px;page-break-inside:avoid;stroke:#000;fill:#000}.md svg.diagram .opendot{fill:#FFF}.md svg.diagram text{stroke:none}@media print{@page{margin:1in 5mm;transform: scale(150%)}}@media print{.md .pagebreak{page-break-after:always;visibility:hidden}}.md a{font-family:Georgia,Palatino,'Times New Roman'}.md h1,.md .tocHeader,.md .nonumberh1{border-bottom:3px solid;font-size:20px;font-weight:bold;}.md h1,.md .nonumberh1{counter-reset: h2 h3 h4 h5 h6}.md h2,.md .nonumberh2{counter-reset: h3 h4 h5 h6;border-bottom:2px solid #999;color:#555;font-weight:bold;font-size:18px;}.md h3,.md h4,.md h5,.md h6,.md .nonumberh3,.md .nonumberh4,.md .nonumberh5,.md .nonumberh6{font-family:Helvetica,Arial,sans-serif;color:#555;font-size:16px;}.md h3{counter-reset:h4 h5 h6}.md h4{counter-reset:h5 h6}.md h5{counter-reset:h6}.md div.table{margin:16px 0 16px 0}.md table{border-collapse:collapse;line-height:140%;page-break-inside:avoid}.md table.table{margin:auto}.md table.calendar{width:100%;margin:auto;font-size:11px;font-family:Helvetica,Arial,sans-serif}.md table.calendar th{font-size:16px}.md .today{background:#ECF8FA}.md .calendar .parenthesized{color:#999;font-style:italic}.md div.tablecaption{text-align:center}.md table.table th{color:#FFF;background-color:#AAA;border:1px solid #888;padding:8px 15px 8px 15px}.md table.table td{padding:5px 15px 5px 15px;border:1px solid #888}.md table.table tr:nth-child(even){background:#EEE}.md pre.tilde{border-top: 1px solid #CCC;border-bottom: 1px solid #CCC;padding: 5px 0 5px 20px;margin:0 0 0 0;background:#FCFCFC;page-break-inside:avoid}.md a.target{width:0px;height:0px;visibility:hidden;font-size:0px;display:inline-block}.md a:link, .md a:visited{color:#38A;text-decoration:none}.md a:link:hover{text-decoration:underline}.md dt{font-weight:700}.md dl>dd{margin-top:-8px; margin-bottom:8px}.md dl>table{margin:35px 0 30px}.md code{white-space:pre-wrap;overflow-wrap:break-word;text-align:left;page-break-inside:avoid}.md .endnote{font-size:13px;line-height:15px;padding-left:10px;text-indent:-10px}.md .bib{padding-left:80px;text-indent:-80px;text-align:left}.markdeepFooter{font-size:9px;text-align:right;padding-top:80px;color:#999}.md .mediumTOC{float:right;font-size:12px;line-height:15px;border-left:1px solid #CCC;padding-left:15px;margin:15px 0px 15px 25px}.md .mediumTOC .level1{font-weight:600}.md .longTOC .level1{font-weight:600;display:block;padding-top:12px;margin:0 0 -20px}.md .shortTOC{text-align:center;font-weight:bold;margin-top:15px;font-size:14px}.md .admonition{position:relative;margin:1em 0;padding:.4rem 1rem;border-radius:.2rem;border-left:2.5rem solid rgba(68,138,255,.4);background-color:rgba(68,138,255,.15);}.md .admonition-title{font-weight:bold;border-bottom:solid 1px rgba(68,138,255,.4);padding-bottom:4px;margin-bottom:4px;margin-left: -1rem;padding-left:1rem;margin-right:-1rem;border-color:rgba(68,138,255,.4)}.md .admonition.tip{border-left:2.5rem solid rgba(50,255,90,.4);background-color:rgba(50,255,90,.15)}.md .admonition.tip::before{content:"\24d8";font-weight:bold;font-size:150%;position:relative;top:3px;color:rgba(26,128,46,.8);left:-2.95rem;display:block;width:0;height:0}.md .admonition.tip>.admonition-title{border-color:rgba(50,255,90,.4)}.md .admonition.warn,.md .admonition.warning{border-left:2.5rem solid rgba(255,145,0,.4);background-color:rgba(255,145,0,.15)}.md .admonition.warn::before,.md .admonition.warning::before{content:"\26A0";font-weight:bold;font-size:150%;position:relative;top:2px;color:rgba(128,73,0,.8);left:-2.95rem;display:block;width:0;height:0}.md .admonition.warn>.admonition-title,.md .admonition.warning>.admonition-title{border-color:rgba(255,145,0,.4)}.md .admonition.error{border-left: 2.5rem solid rgba(255,23,68,.4);background-color:rgba(255,23,68,.15)}.md .admonition.error>.admonition-title{border-color:rgba(255,23,68,.4)}.md .admonition.error::before{content: "\2612";font-family:"Arial";font-size:200%;position:relative;color:rgba(128,12,34,.8);top:-2px;left:-3rem;display:block;width:0;height:0}.md .admonition p:last-child{margin-bottom:0}.md li.checked,.md li.unchecked{list-style:none;overflow:visible;text-indent:-1.2em}.md li.checked:before,.md li.unchecked:before{content:"\2611";display:block;float:left;width:1em;font-size:120%}.md li.unchecked:before{content:"\2610"}</style><style>.md h1::before {
content:counter(h1) " ";
counter-increment: h1;margin-right:10px}.md h2::before {
content:counter(h1) "."counter(h2) " ";
counter-increment: h2;margin-right:10px}.md h3::before {
content:counter(h1) "."counter(h2) "."counter(h3) " ";
counter-increment: h3;margin-right:10px}.md h4::before {
content:counter(h1) "."counter(h2) "."counter(h3) "."counter(h4) " ";
counter-increment: h4;margin-right:10px}.md h5::before {
content:counter(h1) "."counter(h2) "."counter(h3) "."counter(h4) "."counter(h5) " ";
counter-increment: h5;margin-right:10px}.md h6::before {
content:counter(h1) "."counter(h2) "."counter(h3) "."counter(h4) "."counter(h5) "."counter(h6) " ";
counter-increment: h6;margin-right:10px}</style><style>.hljs{display:block;overflow-x:auto;padding:0.5em;background:#fff;color:#000;-webkit-text-size-adjust:none}.hljs-comment{color:#006a00}.hljs-keyword{color:#02E}.hljs-literal,.nginx .hljs-title{color:#aa0d91}.method,.hljs-list .hljs-title,.hljs-tag .hljs-title,.setting .hljs-value,.hljs-winutils,.tex .hljs-command,.http .hljs-title,.hljs-request,.hljs-status,.hljs-name{color:#008}.hljs-envvar,.tex .hljs-special{color:#660}.hljs-string{color:#c41a16}.hljs-tag .hljs-value,.hljs-cdata,.hljs-filter .hljs-argument,.hljs-attr_selector,.apache .hljs-cbracket,.hljs-date,.hljs-regexp{color:#080}.hljs-sub .hljs-identifier,.hljs-pi,.hljs-tag,.hljs-tag .hljs-keyword,.hljs-decorator,.ini .hljs-title,.hljs-shebang,.hljs-prompt,.hljs-hexcolor,.hljs-rule .hljs-value,.hljs-symbol,.hljs-symbol .hljs-string,.hljs-number,.css .hljs-function,.hljs-function .hljs-title,.coffeescript .hljs-attribute{color:#A0C}.hljs-function .hljs-title{font-weight:bold;color:#000}.hljs-class .hljs-title,.smalltalk .hljs-class,.hljs-type,.hljs-typename,.hljs-tag .hljs-attribute,.hljs-doctype,.hljs-class .hljs-id,.hljs-built_in,.setting,.hljs-params,.clojure .hljs-attribute{color:#5c2699}.hljs-variable{color:#3f6e74}.css .hljs-tag,.hljs-rule .hljs-property,.hljs-pseudo,.hljs-subst{color:#000}.css .hljs-class,.css .hljs-id{color:#9b703f}.hljs-value .hljs-important{color:#ff7700;font-weight:bold}.hljs-rule .hljs-keyword{color:#c5af75}.hljs-annotation,.apache .hljs-sqbracket,.nginx .hljs-built_in{color:#9b859d}.hljs-preprocessor,.hljs-preprocessor *,.hljs-pragma{color:#643820}.tex .hljs-formula{background-color:#eee;font-style:italic}.diff .hljs-header,.hljs-chunk{color:#808080;font-weight:bold}.diff .hljs-change{background-color:#bccff9}.hljs-addition{background-color:#baeeba}.hljs-deletion{background-color:#ffc8bd}.hljs-comment .hljs-doctag{font-weight:bold}.method .hljs-id{color:#000}</style><style>div.title { padding-top: 40px; } div.afterTitles { height: 15px; }</style><meta charset="utf-8" lang="en"><style class="fallback">body{visibility:hidden;}</style>
<style>body{line-height: 1.5;} .md div.imagecaption{margin:5px 40px 5px 40px;} .md code{font-size: 90%;}</style>

    </head><body style="visibility: visible;"><span class="md"><p><title>You don't know JAX</title></p><div class="title"> You don't know JAX </div>

<div class="subtitle"> Colin Raffel </div>
<div class="subtitle"> January 16th, 2018 </div>
<div class="subtitle"> <a href="http://colinraffel.com/blog">colinraffel.com/blog</a> </div>
<div class="afterTitles"></div>

<p></p><p>

This brief tutorial covers the basics of <a href="https://github.com/google/jax/">JAX</a>. JAX is a Python library which augments <code>numpy</code> and Python code with <em class="asterisk">function transformations</em> which make it trivial to perform operations common in machine learning programs. Concretely, this makes it simple to write standard Python/<code>numpy</code> code and immediately be able to

</p><p>

</p><ul>
<li class="minus">Compute the derivative of a function via a successor to <a href="https://github.com/HIPS/autograd/">autograd</a>
</li>
<li class="minus">Just-in-time compile a function to run efficiently on an accelerator via <a href="https://www.tensorflow.org/xla/">XLA</a>
</li>
<li class="minus">Automagically vectorize a function, so that e.g. you can process a “batch” of data in parallel</li></ul>

<p></p><p>

In this tutorial, we'll cover each of these transformations in turn by demonstrating their use on one of the core problems of AGI: learning the Exclusive OR (XOR) function with a neural network.

</p><p>

<em class="asterisk">Note - this blog post is available as an interactive Jupyter notebook <a href="https://github.com/craffel/jax-tutorial">here</a>.</em>

</p>
<a class="target" name="jaxisjust%EE%80%90000d%EE%80%90(mostly)">&nbsp;</a><a class="target" name="jaxisjust%EE%80%90000d%EE%80%90(mostly)">&nbsp;</a><a class="target" name="toc1">&nbsp;</a><h1>JAX is just <code>numpy</code> (mostly)</h1>
<p>


At its core, you can think of JAX as augmenting <code>numpy</code> with the machinery required to perform the aforementioned transformations. JAX's augmented numpy lives at <code>jax.numpy</code>. With a few exceptions, you can think of <code>jax.numpy</code> as directly interchangeable with <code>numpy</code>. As a general rule, you should use <code>jax.numpy</code> whenever you plan to use any of JAX's transformations (like computing gradients or just-in-time compiling code) and whenever you want the code to run on an accelerator. You only ever <em class="asterisk">need</em> to use <code>numpy</code> when you're computing something which is not supported by <code>jax.numpy</code>.

</p><pre class="listing tilde"><code><span class="line"><span class="hljs-keyword">import</span> random</span>
<span class="line"><span class="hljs-keyword">import</span> itertools</span>
<span class="line"></span>
<span class="line"><span class="hljs-keyword">import</span> jax</span>
<span class="line"><span class="hljs-keyword">import</span> jax.numpy <span class="hljs-keyword">as</span> np</span>
<span class="line"><span class="hljs-comment"># Current convention is to import original numpy as "onp"</span></span>
<span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> onp</span>
<span class="line"></span>
<span class="line"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function</span></code></pre>
<a class="target" name="background">&nbsp;</a><a class="target" name="background">&nbsp;</a><a class="target" name="toc2">&nbsp;</a><h1>Background</h1>
<p>


As previously mentioned, we will be learning the XOR function with a small neural network. The XOR function takes as input two binary numbers and outputs a binary number, like so:
</p><div class="table"><table class="table"><tbody><tr><th style="text-align:left"> In 1 </th><th style="text-align:left"> In 2 </th><th style="text-align:left"> Out </th></tr>
<tr><td style="text-align:left"> 0 </td><td style="text-align:left"> 0 </td><td style="text-align:left"> 0 </td></tr>
<tr><td style="text-align:left"> 0 </td><td style="text-align:left"> 1 </td><td style="text-align:left"> 1 </td></tr>
<tr><td style="text-align:left"> 1 </td><td style="text-align:left"> 0 </td><td style="text-align:left"> 1 </td></tr>
<tr><td style="text-align:left"> 1 </td><td style="text-align:left"> 1 </td><td style="text-align:left"> 0 </td></tr>
</tbody></table></div>

<p></p><p>

We'll use a neural network with a single hidden layer with 3 neurons and a hyperbolic tangent nonlinearity, trained with the cross-entropy loss via stochastic gradient descent. Let's implement this model and loss function. Note that the code is exactly as you'd write in standard <code>numpy</code>.

</p><pre class="listing tilde"><code><span class="line"><span class="hljs-comment"># Sigmoid nonlinearity</span></span>
<span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(x)</span>:</span></span>
<span class="line">    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))</span>
<span class="line"></span>
<span class="line"><span class="hljs-comment"># Computes our network's output</span></span>
<span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">net</span><span class="hljs-params">(params, x)</span>:</span></span>
<span class="line">    w1, b1, w2, b2 = params</span>
<span class="line">    hidden = np.tanh(np.dot(w1, x) + b1)</span>
<span class="line">    <span class="hljs-keyword">return</span> sigmoid(np.dot(w2, hidden) + b2)</span>
<span class="line"></span>
<span class="line"><span class="hljs-comment"># Cross-entropy loss</span></span>
<span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loss</span><span class="hljs-params">(params, x, y)</span>:</span></span>
<span class="line">    out = net(params, x)</span>
<span class="line">    cross_entropy = -y * np.log(out) - (<span class="hljs-number">1</span> - y)*np.log(<span class="hljs-number">1</span> - out)</span>
<span class="line">    <span class="hljs-keyword">return</span> cross_entropy</span>
<span class="line"></span>
<span class="line"><span class="hljs-comment"># Utility function for testing whether the net produces the correct</span></span>
<span class="line"><span class="hljs-comment"># output for all possible inputs</span></span>
<span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_all_inputs</span><span class="hljs-params">(inputs, params)</span>:</span></span>
<span class="line">    predictions = [int(net(params, inp) &gt; <span class="hljs-number">0.5</span>) <span class="hljs-keyword">for</span> inp <span class="hljs-keyword">in</span> inputs]</span>
<span class="line">    <span class="hljs-keyword">for</span> inp, out <span class="hljs-keyword">in</span> zip(inputs, predictions):</span>
<span class="line">        print(inp, <span class="hljs-string">'-&gt;'</span>, out)</span>
<span class="line">    <span class="hljs-keyword">return</span> (predictions == [onp.bitwise_xor(*inp) <span class="hljs-keyword">for</span> inp <span class="hljs-keyword">in</span> inputs])</span></code></pre><p>

As mentioned above, there are some places where we want to use standard <code>numpy</code> rather than <code>jax.numpy</code>. One of those places is with parameter initialization. We'd like to initialize our parameters randomly before we train our network, which is not an operation for which we need derivatives or compilation. JAX uses its own <code>jax.random</code> library instead of <code>numpy.random</code> which provides better support for reproducibility (seeding) across different transformations. Since we don't need to transform the initialization of parameters in any way, it's simplest just to use standard <code>numpy.random</code> instead of <code>jax.random</code> here.

</p><pre class="listing tilde"><code><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initial_params</span><span class="hljs-params">()</span>:</span></span>
<span class="line">    <span class="hljs-keyword">return</span> [</span>
<span class="line">        onp.random.randn(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>),  <span class="hljs-comment"># w1</span></span>
<span class="line">        onp.random.randn(<span class="hljs-number">3</span>),  <span class="hljs-comment"># b1</span></span>
<span class="line">        onp.random.randn(<span class="hljs-number">3</span>),  <span class="hljs-comment"># w2</span></span>
<span class="line">        onp.random.randn(),  <span class="hljs-comment">#b2</span></span>
<span class="line">    ]</span></code></pre>
<a class="target" name="%EE%80%90000s%EE%80%90">&nbsp;</a><a class="target" name="%EE%80%90000s%EE%80%90">&nbsp;</a><a class="target" name="toc3">&nbsp;</a><h1><code>jax.grad</code></h1>
<p>


The first transformation we'll use is <code>jax.grad</code>. <code>jax.grad</code> takes a function and returns a new function which computes the gradient of the original function. By default, the gradient is taken with respect to the first argument; this can be controlled via the <code>argnums</code> argument to <code>jax.grad</code>. To use gradient descent, we want to be able to compute the gradient of our loss function with respect to our neural network's parameters. For this, we'll simply use <code>jax.grad(loss)</code> which will give us a function we can call to get these gradients.

</p><pre class="listing tilde"><code><span class="line">loss_grad = jax.grad(loss)</span>
<span class="line"></span>
<span class="line"><span class="hljs-comment"># Stochastic gradient descent learning rate</span></span>
<span class="line">learning_rate = <span class="hljs-number">1.</span></span>
<span class="line"><span class="hljs-comment"># All possible inputs</span></span>
<span class="line">inputs = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])</span>
<span class="line"></span>
<span class="line"><span class="hljs-comment"># Initialize parameters randomly</span></span>
<span class="line">params = initial_params()</span>
<span class="line"></span>
<span class="line"><span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> itertools.count():</span>
<span class="line">    <span class="hljs-comment"># Grab a single random input</span></span>
<span class="line">    x = inputs[onp.random.choice(inputs.shape[<span class="hljs-number">0</span>])]</span>
<span class="line">    <span class="hljs-comment"># Compute the target output</span></span>
<span class="line">    y = onp.bitwise_xor(*x)</span>
<span class="line">    <span class="hljs-comment"># Get the gradient of the loss for this input/output pair</span></span>
<span class="line">    grads = loss_grad(params, x, y)</span>
<span class="line">    <span class="hljs-comment"># Update parameters via gradient descent</span></span>
<span class="line">    params = [param - learning_rate * grad</span>
<span class="line">              <span class="hljs-keyword">for</span> param, grad <span class="hljs-keyword">in</span> zip(params, grads)]</span>
<span class="line">    <span class="hljs-comment"># Every 100 iterations, check whether we've solved XOR</span></span>
<span class="line">    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> n % <span class="hljs-number">100</span>:</span>
<span class="line">        print(<span class="hljs-string">'Iteration {}'</span>.format(n))</span>
<span class="line">        <span class="hljs-keyword">if</span> test_all_inputs(inputs, params):</span>
<span class="line">            <span class="hljs-keyword">break</span></span></code></pre><pre class="listing backtick"><code><span class="line">Iteration 0</span>
<span class="line">[0 0] -&gt; 1</span>
<span class="line">[0 1] -&gt; 0</span>
<span class="line">[1 0] -&gt; 1</span>
<span class="line">[1 1] -&gt; 1</span>
<span class="line">Iteration 100</span>
<span class="line">[0 0] -&gt; 0</span>
<span class="line">[0 1] -&gt; 1</span>
<span class="line">[1 0] -&gt; 1</span>
<span class="line">[1 1] -&gt; 0</span></code></pre>
<a class="target" name="%EE%80%90000y%EE%80%90">&nbsp;</a><a class="target" name="%EE%80%90000y%EE%80%90">&nbsp;</a><a class="target" name="toc4">&nbsp;</a><h1><code>jax.jit</code></h1>
<p>


While carefully-written <code>numpy</code> code can be reasonably performant, for modern machine learning we want our code to run as fast as possible. This often involves running our code on different “accelerators” like GPUs or TPUs. JAX provides a JIT (just-in-time) compiler which takes a standard Python/<code>numpy</code> function and compiles it to run efficiently on an accelerator. Compiling a function also avoids the overhead of the Python interpreter, which helps whether or not you're using an accelerator. In total, <code>jax.jit</code> can dramatically speed-up your code with essentially no coding overhead - you just ask JAX to compile the function for you. Even our tiny neural network can see a pretty dramatic speedup when using <code>jax.jit</code>:

</p><pre class="listing tilde"><code><span class="line"><span class="hljs-comment"># Time the original gradient function</span></span>
<span class="line">%timeit loss_grad(params, x, y)</span>
<span class="line">loss_grad = jax.jit(jax.grad(loss))</span>
<span class="line"><span class="hljs-comment"># Run once to trigger JIT compilation</span></span>
<span class="line">loss_grad(params, x, y)</span>
<span class="line">%timeit loss_grad(params, x, y)</span></code></pre><pre class="listing backtick"><code><span class="line">10 loops, best of 3: 13.1 ms per loop</span>
<span class="line">1000 loops, best of 3: 862 µs per loop</span></code></pre><p>

Note that JAX allows us to aribtrarily chain together transformations - first, we took the gradient of <code>loss</code> using <code>jax.grad</code>, then we just-in-time compiled it using <code>jax.jit</code>. This is one of the things that makes JAX extra powerful — apart from chaining <code>jax.jit</code> and <code>jax.grad</code>, we could also e.g. apply <code>jax.grad</code> multiple times to get higher-order derivatives. To make sure that training the neural network still works after compilation, let's train it again. Note that the code for training has not changed whatsoever.

</p><pre class="listing tilde"><code><span class="line">params = initial_params()</span>
<span class="line"></span>
<span class="line"><span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> itertools.count():</span>
<span class="line">    x = inputs[onp.random.choice(inputs.shape[<span class="hljs-number">0</span>])]</span>
<span class="line">    y = onp.bitwise_xor(*x)</span>
<span class="line">    grads = loss_grad(params, x, y)</span>
<span class="line">    params = [param - learning_rate * grad</span>
<span class="line">              <span class="hljs-keyword">for</span> param, grad <span class="hljs-keyword">in</span> zip(params, grads)]</span>
<span class="line">    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> n % <span class="hljs-number">100</span>:</span>
<span class="line">        print(<span class="hljs-string">'Iteration {}'</span>.format(n))</span>
<span class="line">        <span class="hljs-keyword">if</span> test_all_inputs(inputs, params):</span>
<span class="line">            <span class="hljs-keyword">break</span></span></code></pre><pre class="listing backtick"><code><span class="line">Iteration 0</span>
<span class="line">[0 0] -&gt; 1</span>
<span class="line">[0 1] -&gt; 1</span>
<span class="line">[1 0] -&gt; 1</span>
<span class="line">[1 1] -&gt; 1</span>
<span class="line">Iteration 100</span>
<span class="line">[0 0] -&gt; 0</span>
<span class="line">[0 1] -&gt; 1</span>
<span class="line">[1 0] -&gt; 1</span>
<span class="line">[1 1] -&gt; 0</span></code></pre>
<a class="target" name="%EE%80%90001a%EE%80%90">&nbsp;</a><a class="target" name="%EE%80%90001a%EE%80%90">&nbsp;</a><a class="target" name="toc5">&nbsp;</a><h1><code>jax.vmap</code></h1>
<p>


An astute reader may have noticed that we have been training our neural network on a single example at a time. This is “true” stochastic gradient descent; in practice, when training modern machine learning models we perform “minibatch” gradient descent where we average the loss gradients over a mini-batch of examples at each step of gradient descent. JAX provides <code>jax.vmap</code>, which is a transformation which automatically “vectorizes” a function. What this means is that it allows you to compute the output of a function in parallel over some axis of the input. For us, this means we can apply the <code>jax.vmap</code> function transformation and immediately get a version of our loss function gradient which is amenable to using a minibatch of examples.

</p><p>

<code>jax.vmap</code> takes in additional arguments:

</p><p>

</p><ul>
<li class="minus"><code>in_axes</code> is a tuple or integer which tells JAX over which axes the function's arguments should be parallelized. The tuple should have the same length as the number of arguments of the function being <code>vmap</code>'d, or should be an integer when there is only one argument. In our example, we'll use <code>(None, 0, 0)</code>, meaning “don't parallelize over the first argument (<code>params</code>), and parallelize over the first (zeroth) dimension of the second and third arguments (<code>x</code> and <code>y</code>)".
</li>
<li class="minus"><code>out_axes</code> is analogous to <code>in_axes</code>, except it specifies which axes of the function's output to parallelize over. In our case, we'll use <code>0</code>, meaning to parallelize over the first (zeroth) dimension of the function's sole output (the loss gradients).</li></ul>

<p></p><p>

Note that we will have to change the training code a little bit - we need to grab a batch of data instead of a single example at a time, and we need to average the gradients over the batch before applying them to update the parameters.

</p><pre class="listing tilde"><code><span class="line">loss_grad = jax.jit(jax.vmap(jax.grad(loss), in_axes=(<span class="hljs-keyword">None</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), out_axes=<span class="hljs-number">0</span>))</span>
<span class="line"></span>
<span class="line">params = initial_params()</span>
<span class="line"></span>
<span class="line">batch_size = <span class="hljs-number">100</span></span>
<span class="line"></span>
<span class="line"><span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> itertools.count():</span>
<span class="line">    <span class="hljs-comment"># Generate a batch of inputs</span></span>
<span class="line">    x = inputs[onp.random.choice(inputs.shape[<span class="hljs-number">0</span>], size=batch_size)]</span>
<span class="line">    y = onp.bitwise_xor(x[:, <span class="hljs-number">0</span>], x[:, <span class="hljs-number">1</span>])</span>
<span class="line">    <span class="hljs-comment"># The call to loss_grad remains the same!</span></span>
<span class="line">    grads = loss_grad(params, x, y)</span>
<span class="line">    <span class="hljs-comment"># Note that we now need to average gradients over the batch</span></span>
<span class="line">    params = [param - learning_rate * np.mean(grad, axis=<span class="hljs-number">0</span>)</span>
<span class="line">              <span class="hljs-keyword">for</span> param, grad <span class="hljs-keyword">in</span> zip(params, grads)]</span>
<span class="line">    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> n % <span class="hljs-number">100</span>:</span>
<span class="line">        print(<span class="hljs-string">'Iteration {}'</span>.format(n))</span>
<span class="line">        <span class="hljs-keyword">if</span> test_all_inputs(inputs, params):</span>
<span class="line">            <span class="hljs-keyword">break</span></span></code></pre><pre class="listing backtick"><code><span class="line">Iteration 0</span>
<span class="line">[0 0] -&gt; 0</span>
<span class="line">[0 1] -&gt; 0</span>
<span class="line">[1 0] -&gt; 0</span>
<span class="line">[1 1] -&gt; 0</span>
<span class="line">Iteration 100</span>
<span class="line">[0 0] -&gt; 0</span>
<span class="line">[0 1] -&gt; 1</span>
<span class="line">[1 0] -&gt; 1</span>
<span class="line">[1 1] -&gt; 0</span></code></pre>
<a class="target" name="pointers">&nbsp;</a><a class="target" name="pointers">&nbsp;</a><a class="target" name="toc6">&nbsp;</a><h1>Pointers</h1>
<p>


That's all we'll be covering in this short tutorial, but this actually covers a great deal of JAX. Since JAX is mostly <code>numpy</code> and Python, you can leverage your existing knowledge instead of having to learn a fundamentally new framework or paradigm. For additional resources, check the <a href="https://github.com/google/jax/tree/master/notebooks">notebooks</a> and <a href="https://github.com/google/jax/tree/master/examples">examples</a> directories on <a href="https://github.com/google/jax">JAX's GitHub</a>.

</p><p>

<script>window.markdeepOptions={}; window.markdeepOptions.tocStyle=&ldquo;none&rdquo;;</script>

</p><p>

</p></span><div class="markdeepFooter"><i>formatted by <a href="http://casual-effects.com/markdeep" style="color:#999">Markdeep&nbsp;1.03&nbsp;&nbsp;</a></i><div style="display:inline-block;font-size:13px;font-family:'Times New Roman',serif;vertical-align:middle;transform:translate(-3px,-1px)rotate(135deg);">✒</div></div></body></html>
