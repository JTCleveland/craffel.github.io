<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
        <title>Colin Raffel: Research Statement</title>
        <link rel="stylesheet" type="text/css" href="../cv.css">
        <style type="text/css">
                ol.references {
                        list-style: none;
                        counter-reset: citations;
                        padding-left: 2.2em;
                        margin-block-start: 0.5em;
                        margin-block-end: 0.5em;
                }
                li.reference::before {
                        content: "[" counter(citations) "] ";
                        margin-left: -2.2em;
                        width: 2.2em;
                        display: inline-block;
                        white-space: nowrap;
                        font-family: -apple-system, BlinkMacSystemFont, "Roboto", Helvetica, sans-serif;
                        font-size: 85%;
                        color: hsla(206, 90%, 20%, 0.7);
                        line-height: 1.1em;
                        position: relative;
                        top: -1px;
                }
                li.reference {
                        counter-increment: citations;
                        margin-bottom: 0.5em;
                }
                li.example {
                        margin: 0px 0px 0.5em -0.5em;
                }
                ul {
                        margin-block-start: 0.5em;
                        margin-block-end: 0.5em;
                }
                .cite::before {
                        content: "[";
                }
                .cite::after {
                        content: "]";
                }
                .cite {
                        white-space: nowrap;
                        font-family: -apple-system, BlinkMacSystemFont, "Roboto", Helvetica, sans-serif;
                        font-size: 85%;
                        color: hsla(206, 90%, 20%, 0.7);
                        display: inline-block;
                        line-height: 1.1em;
                        text-align: center;
                        position: relative;
                        top: -1px;
                }
        </style>
    </head>
    </head>
    <body>

            <h1 id="name">Colin Raffel</h1> &nbsp; <span class="docname">Research Statement</span>

            <div class="vline"></div>

<p>
In my research on machine learning, I develop models which can discover, represent, and manipulate the structure of sequential data.</p>
<p>What currently prevents us from translating from Hakka to Kinyarwanda, official languages in Taiwan and Rwanda respectively with tens of millions of speakers? No ground-truth translations exist for either language, which may be why they are not supported by Google Translate. Ideally, we could utilize the thousands of Wikipedia articles written in each language as sources of unlabeled data. A given Wikipedia article can be considered as some underlying, or <i>latent</i>, information conveyed in a particular language. In some cases, generative models can uncover and control this kind of data generation process using only unlabeled data. This control could allow translation from Hakka to Kinyarwanda by changing the language of a message without altering its latent content. In fact, this kind of structure exists in many domains -- for example, a spoken utterance is a latent message communicated by a speaker; a piece of music is a latent score performed by a musician. My long-term goal is to devise methods to disentangle the underlying content of a sequence from the way it was expressed. I attack this problem by developing generative modeling and representation learning techniques for sequential data. In the following, I present my past work and future plans towards building learning algorithms which can uncover structure in sequences.</p>
<h2>Learning Efficient Representations for Sequence Retrieval</h2>
<p>Defining a reliable similarity measure for sequences is complicated by the fact that sequences vary in length and can have a high correlation between consecutive sequence elements. Warping-based similarity scores such as edit distance or dynamic time warping can be effective but are prohibitively expensive for comparing long sequences. To address these issues, <strong>I developed representation learning techniques that allow sequential data to be compared efficiently and accurately.</strong></p>
<p>The first method maps sequences of continuous-valued feature vectors to shorter sequences of binary vectors using a siamese convolutional network <span class="cite">1</span>. The network’s objective encourages the resulting binary vectors to be similar for groups of feature vectors from matching pairs of sequences. The resulting shortened binary vector sequences can be reliably compared while providing many orders of magnitude in speedup. However, this approach remains reliant on a warping-based similarity score. A more efficient solution is to map entire variable-length sequences to fixed-length embeddings whose distance captures sequence similarity. To produce such a mapping, I developed a novel feed-forward attention mechanism <span class="cite">2</span> which computes an embedding as the weighted average of entries of a sequence <span class="cite">3</span>. The entire model is trained end-to-end so that the resulting embeddings are close for matching sequences and distant otherwise. While distances between these embeddings are less discriminant than those produced by warping-based similarity scores, comparing the embeddings is dramatically faster, opening the possibility of pre-filtering for more expensive methods.</p>
<p>Applications of machine learning to music are held back by the expense of creating ground-truth data, but scores which have been matched and aligned to an audio recording of a song can provide valuable annotations <span class="cite">4</span>. To test out these sequence comparison approaches, I focused on the task of creating a dataset of scores matched to corresponding audio recordings in a large database of music. Since the query must be compared to every entry in the database, efficient comparison of each possible match is crucial. I used an iterative approach which first compares embeddings, then binary sequences, and finally performs an expensive fine-grained alignment and match confirmation <span class="cite">5</span>. <strong>The combined application of these methods resulted in three orders of magnitude of speedup and produced a collection of score-audio pairs which is orders of magnitude larger than what was previously available.</strong> I released this collection as the Lakh MIDI Dataset <span class="cite">6</span>, and it has already been used in dozens of publications.</p>
<h2>Generative Models for Sequences</h2>
<p>Sequence-to-sequence generative models have produced state-of-the-art results on machine translation, speech recognition, and other sequence transduction problems. A crucial architectural innovation for these models is attention, which provides direct access to the input sequence as the output is being generated. Standard attention mechanisms have a quadratic time complexity and are not applicable to online problems where the input is being transduced as it is being generated. To follow up on my work on feed-forward attention for efficient sequence comparison, <strong>I devised the first online and linear-time attention mechanisms for sequence-to-sequence models.</strong> In many sequence transduction problems, once a given entry in the input sequence has been attended to, the model does not attend to any prior input entries at subsequent output timesteps. To leverage this property, we developed a “monotonic” attention mechanism which makes a discrete choice between ingesting another input element or outputting a new token <span class="cite">7</span>. We formulated a dynamic program which computes the expected attention location in order to allow for simple gradient-based training. Across many sequence-to-sequence problems, we achieved online and linear-time sequence transduction with only a modest decrease in accuracy compared to a standard offline attention mechanism. We closed this performance gap on speech recognition in follow-up work by developing a way for the model to attend to a small fixed-size chunk of the input sequence at each output timestep <span class="cite">8</span>. We also developed reinforcement learning techniques which allow discrete attention decisions to be used during training <span class="cite">9</span>.</p>
<p>Another common class of sequence-to-sequence models are autoencoders which are trained to reconstruct an input sequence after mapping it to a fixed-length latent encoding. We utilized this framework to develop a generative model of 16-bar melodies and drum loops called "MusicVAE" <span class="cite">10</span>. Crucial to our approach was the use of a hierarchical architecture which first outputs embeddings for each bar and then outputs the notes in each bar independently. This encourages the model to capture the global structure of the sequence in its latent encoding and explicitly reflects the natural hierarchical structure in music. With this architectural improvement, <strong>our model can generate realistic samples, provide control over the generative process via its latent space, and convincingly interpolate between sequences.</strong> In follow-up work, we showed that this model can be conditioned on chord and instrumentation information, which allows for songs with specific chord progressions to be generated <span class="cite">11</span>.</p>
<p>In models like MusicVAE, the latent representation is treated as a random variable which is distributed according to a simple probability distribution. The characteristics of this prior distribution are a major factor in what allows these models to perform semantic manipulation like interpolating between datapoints. In recent work <span class="cite">12</span>, we investigated whether it was possible to train an autoencoder to perform this kind of manipulation without treating the latent representation as a random variable. To do so, we introduced a constraint which encourages the autoencoder to produce realistic outputs when reconstructing linear combinations of latent encodings. Importantly, we showed that adding this constraint to the autoencoder resulted in a learned representation which performed better on downstream tasks such as classification and clustering. <strong>Our findings suggest that latent representations which facilitate data manipulation may also provide better representations for downstream tasks.</strong></p>
<h2>Disentangling the Medium and the Message</h2>
<p>My past work shows how representation learning and generative modeling can unlock many important applications involving sequential data. However, current models are not able to discover and provide control over the data generation process for sequences with long-term structure. Some immediate applications that become possible after solving this issue include:</p>
<ul>
<li class="example">Underlying a spoken utterance and its transcription is the message being communicated. The utterance is a realization of this message by a given speaker with a certain speaking style, whereas the transcription represents the message as text. In other words, both the utterance and the transcription have the same intrinsic content. An effective generative model may be able to leverage unlabeled, unpaired data to learn to "translate" speech to text and text to speech by jointly modeling both with a shared representation.</li>
<li class="example">A generative model of paragraphs that provides control over text attributes via its latent representation would allow for "Photoshop for text.” For example, a writer could perform useful manipulations such as "make this paragraph happier," "rewrite this paragraph in the style of Shakespeare,” or "shorten this paragraph without changing its meaning.”</li>
<li class="example">An agent that can accurately predict what comes next in an environment can use this information to better determine its optimal next action. Deep autoregressive models are remarkably powerful at next-step prediction, but in order to utilize them effectively, an agent needs access to a representation from which it can easily deduce the best action. Furthermore, this representation should ideally encode information relevant to the agent's task as opposed to irrelevant factors about the environment. This provides a direct link between learning useful representations for sequences and model-based reinforcement learning.</li>
</ul>
<p>What is holding us back from utilizing generative models for these applications? Deep convolutional architectures can provide a natural "prior" for images where successive layers produce a hierarchy of fine-to-coarse concepts, from edges to simple shapes to objects. Many sequences also exhibit hierarchical structure -- for example, a word is a sequence of characters, a sentence is made up of words, a paragraph consists of a few sentences, and so on. However, architectures commonly used for sequences have not been shown to capture this hierarchical structure. While hard-coding task-specific knowledge into the architecture may be possible in some domains, ideally a model would learn to discover this structure on its own without supervision. One promising means of achieving this behavior is to allow a sequential model to decide when to update its internal state, as is done in the monotonic attention mechanism. We developed some preliminary work in this direction by introducing a "subsampling mechanism" which decides which elements in the input sequence to include in its output <span class="cite">13</span>.</p>
<p>Separately, while constraining latent codes to follow a simple continuous distribution can produce a useful representation, in many problems we might expect the data to have discrete or otherwise complex structure. For example, a scene consists of a discrete set of objects placed at continuous locations with continuous lighting conditions. In other tasks the latent factors are best represented as a sequence - for example, the words in a spoken utterance are themselves a sequence of discrete tokens. Similarly, the ingest/emit actions performed by the “hard” attention mechanisms we developed produce a sequence of discrete latent variables. While a simple continuous latent distribution may be able to capture all of these factors, structured latent spaces would provide better interpretability, discriminability, and control. I will enable the use of this kind of latent representation by continuing to investigate gradient estimation and inference for discrete and sequential latent variables.</p>
<h2>Philosophy</h2>
<p>My work as a researcher involves not only making new discoveries but also improving experimental practice by creating and contributing to open-source software projects and conducting meta-studies on benchmarking. During my Ph.D. I led the development of mir_eval, a unified implementation of metrics used to evaluate machine learning models on music tasks which has since become standard in the field. In our paper introducing mir_eval <span class="cite">14</span>, we compared it to existing software packages used for benchmarking and found a striking disparity between each implementation. In a similar vein, some colleagues and I recently conducted a meta-study of evaluation methods for semi-supervised learning and demonstrated how they poorly reflect real-world use cases <span class="cite">15</span>.</p>
<p>While writing the software for my Ph.D. thesis, I also helped create and develop pretty_midi (for MIDI analysis) <span class="cite">16</span>, librosa (for audio analysis) <span class="cite">17</span>, and lasagne (for training neural networks) <span class="cite">18</span>, which were critical to my doctoral work and have since become widely used. My work on lasagne led me to be an active contributor to Theano <span class="cite">19</span>, which introduced the paradigm of symbolic computation graphs later used in popular libraries like TensorFlow. I also strive to make all of my experimental code available; all of the publications discussed in this research statement were accompanied with a code release.</p>
<p>In my future lab, I envision students working on fundamental problems in generative modeling and representation learning in order to unlock innovative uses of sequential data. Our work will produce research papers, open-source software, and real-world applications. My past work as a researcher, mentor, and creator of widely-used tools for research provides the skill set needed to make this agenda a success.</p>
<h2>References</h2>
<ol class="references">
<li class="reference">“Large-Scale Content-Based Matching of MIDI and Audio Files”, <strong>Colin Raffel</strong> and Daniel P. W. Ellis. 16th International Society for Music Information Retrieval Conference (ISMIR), 2015.</li>
<li class="reference">“Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems”, <strong>Colin Raffel</strong> and Daniel P. W. Ellis. 4th International Conference on Learning Representations Workshop (ICLR), 2016.</li>
<li class="reference">“Pruning Subsequence Search with Attention-Based Embedding”, <strong>Colin Raffel</strong> and Daniel P. W. Ellis. 41st IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016.</li>
<li class="reference">“Extracting Ground Truth Information from MIDI Files: A MIDIfesto”, <strong>Colin Raffel</strong> and Daniel P. W. Ellis. 17th International Society for Music Information Retrieval Conference (ISMIR), 2016.</li>
<li class="reference">“Optimizing DTW-Based Audio-to-MIDI Alignment and Matching”, <strong>Colin Raffel</strong> and Daniel P. W. Ellis. 41st IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016.</li>
<li class="reference">“Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching”, <strong>Colin Raffel</strong>. Columbia University Ph.D. Thesis, 2016.</li>
<li class="reference">“Online and Linear-Time Attention by Enforcing Monotonic Alignments”, <strong>Colin Raffel</strong>, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, and Douglas Eck. 34th International Conference on Machine Learning (ICML), 2017.</li>
<li class="reference">“Monotonic Chunkwise Attention”, Chung-Cheng Chiu* and <strong>Colin Raffel</strong>*. 6th International Conference on Learning Representations (ICLR), 2018.</li>
<li class="reference">“Learning Hard Alignments with Variational Inference”, Dieterich Lawson*, George Tucker*, Chung-Cheng Chiu*, <strong>Colin Raffel</strong>, Kevin Swersky, and Navdeep Jaitly. 43rd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.</li>
<li class="reference">“A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music”, Adam Roberts, Jesse Engel, <strong>Colin Raffel</strong>, Curtis Hawthorne, and Douglas Eck. 35th International Conference on Machine Learning (ICML), 2018.</li>
<li class="reference">“Learning a Latent Space of Multitrack Measures”, Ian Simon, Adam Roberts, <strong>Colin Raffel</strong>, Jesse Engel, Curtis Hawthorne, and Douglas Eck. arXiv:1806.00195, 2018.</li>
<li class="reference">“Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer”, David Berthelot*, <strong>Colin Raffel</strong>*, Aurko Roy and Ian Goodfellow.  arXiv:1807.07543, 2018.</li>
<li class="reference">“Training a Subsampling Mechanism in Expectation”, <strong>Colin Raffel</strong> and Dieterich Lawson. 5th International Conference on Learning Representations Workshop (ICLR), 2017.</li>
<li class="reference">“mir_eval: A Transparent Implementation of Common MIR Metrics”, <strong>Colin Raffel</strong>, Brian McFee, Eric J. Humphrey, Justin Salamon, Oriol Nieto, Dawen Liang, and Daniel P. W. Ellis. 15th International Society for Music Information Retrieval Conference (ISMIR), 2014.</li>
<li class="reference">“Realistic Evaluation of Deep Semi-Supervised Learning Algorithms”, Avital Oliver*, Augustus Odena*, <strong>Colin Raffel</strong>*, Ekin D. Cubuk, and Ian J. Goodfellow. Neural Information Processing Systems 31 (NIPS), 2018.</li>
<li class="reference">“Intuitive Analysis, Creation and Manipulation of MIDI Data with pretty_midi”, <strong>Colin Raffel</strong> and Daniel P. W. Ellis. 15th International Society for Music Information Retrieval Conference Late Breaking and Demo Papers, 2014.</li>
<li class="reference">“librosa: Audio and Music Signal Analysis in Python”, Brian McFee, <strong>Colin Raffel</strong>, Dawen Liang, Daniel P. W. Ellis, Matt McVicar, and Oriol Nieto. 14th Python in Science Conference (SciPy), 2015.</li>
<li class="reference">“Lasagne: First Release”, Sander Dieleman, Jan Schlüter, <strong>Colin Raffel</strong>, Eben Olson, and others. Zenodo.27878, 2015.</li>
<li class="reference">"Theano: A Python framework for fast computation of mathematical expressions", The Theano Development Team. arXiv:1605.02688, 2016.</li>
</ol>
</body>
</html>
